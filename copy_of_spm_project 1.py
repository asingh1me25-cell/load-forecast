# -*- coding: utf-8 -*-
"""Copy of SPM_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIqVteIA0bI0WKT5S2mp1CUov-dbf5PY
"""

from google.colab import drive
drive.mount('/content/gdrive')



import pandas as pd
df = pd.read_excel('/content/hourlyLoadDataIndia.xlsx')
print(df.head())

print(f"Shape: {df.shape}")

print(f"Columns: {list(df.columns)}")

"""DATA VISUALIZATION"""

print("="*100)
print("EXPLORATORY DATA ANALYSIS - COMPREHENSIVE")
print("="*100)
print("\n[1] DATASET SHAPE & SIZE")
print("-"*100)
print(f"Rows: {len(df):,}")
print(f"Columns: {len(df.columns)}")
print(f"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB")
print(f"Data types:\n{df.dtypes}\n")
print("[2] TIME PERIOD & COVERAGE")
print("-"*100)
print(f"Start date: {df['datetime'].min()}")
print(f"End date: {df['datetime'].max()}")
print(f"Total days: {(df['datetime'].max() - df['datetime'].min()).days:,}")
print(f"Total years: {(df['datetime'].max() - df['datetime'].min()).days / 365.25:.2f}")
print(f"Hourly records: {len(df):,}")
print("[3] COLUMN NAMES")
print("-"*100)
for i, col in enumerate(df.columns, 1):
    print(f"  {i}. {col}")
print()

print("[5] NATIONAL HOURLY DEMAND - DETAILED STATISTICS")
print("-"*100)
demand = df['National Hourly Demand']

print(f"Count: {demand.count():,}")
print(f"Mean: {demand.mean():,.2f} MWh")
print(f"Median: {demand.median():,.2f} MWh")
print(f"Std Dev: {demand.std():,.2f} MWh")
print(f"Variance: {demand.var():,.2f} MWh²")
print(f"Min: {demand.min():,.2f} MWh")
print(f"Max: {demand.max():,.2f} MWh")
print(f"Range: {demand.max() - demand.min():,.2f} MWh")
print(f"IQR (25%-75%): {demand.quantile(0.75) - demand.quantile(0.25):,.2f} MWh")

print("\nPercentiles:")
for pct in [1, 5, 10, 25, 50, 75, 90, 95, 99]:
    print(f"  {pct}%: {demand.quantile(pct/100):,.2f} MWh")

print("\nDistribution shape:")
print(f"  Skewness: {stats.skew(demand):.4f}")
print(f"  Kurtosis: {stats.kurtosis(demand):.4f}")
cv = (demand.std() / demand.mean()) * 100
print(f"  Coefficient of Variation: {cv:.2f}%\n")

print("[6] REGIONAL DEMAND ANALYSIS")
print("-"*100)

regions = {
    'North': 'Northen Region Hourly Demand',
    'West': 'Western Region Hourly Demand',
    'East': 'Eastern Region Hourly Demand',
    'South': 'Southern Region Hourly Demand',
    'Northeast': 'North-Eastern Region Hourly Demand'
}
total = df['National Hourly Demand'].mean()
print(f"{'Region':<12} {'Avg (MWh)':<15} {'Min':<15} {'Max':<15} {'% of Total':<12}")
print("-"*70)
for region_name, region_col in regions.items():
    avg = df[region_col].mean()
    min_val = df[region_col].min()
    max_val = df[region_col].max()
    pct = (avg / total) * 100
    print(f"{region_name:<12} {avg:>12,.0f}  {min_val:>12,.0f}  {max_val:>12,.0f}  {pct:>10.1f}%")
print("-"*70)

df['hour'] = df['datetime'].dt.hour
df['dayofweek'] = df['datetime'].dt.dayofweek
df['month'] = df['datetime'].dt.month
df['year'] = df['datetime'].dt.year
df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)

print("[7] TEMPORAL PATTERNS")
print("-"*100)

print("A. HOURLY PATTERN")
hourly_stats = df.groupby('hour')['National Hourly Demand'].agg(['mean', 'std', 'min', 'max'])
print(hourly_stats.to_string())

peak_hour = hourly_stats['mean'].idxmax()
low_hour = hourly_stats['mean'].idxmin()
print(f"\n→ Peak hour: {peak_hour}:00 ({hourly_stats.loc[peak_hour, 'mean']:,.0f} MWh)")
print(f"→ Lowest hour: {low_hour}:00 ({hourly_stats.loc[low_hour, 'mean']:,.0f} MWh)")

print("\nB. DAILY PATTERN (WEEKDAY vs WEEKEND)")
days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
daily_stats = df.groupby('dayofweek')['National Hourly Demand'].agg(['mean', 'std'])
daily_stats.index = days
print(daily_stats.to_string())

weekday_avg = df[df['is_weekend'] == 0]['National Hourly Demand'].mean()
weekend_avg = df[df['is_weekend'] == 1]['National Hourly Demand'].mean()
diff = ((weekday_avg - weekend_avg) / weekday_avg) * 100
print(f"\n→ Weekday avg: {weekday_avg:,.0f} MWh")
print(f"→ Weekend avg: {weekend_avg:,.0f} MWh")
print(f"→ Difference: {diff:.2f}% (lower on weekends)")

print("\nC. MONTHLY PATTERN (SEASONAL)")
months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
monthly_stats = df.groupby('month')['National Hourly Demand'].agg(['mean', 'std', 'min', 'max'])
monthly_stats.index = months
print(monthly_stats.to_string())

peak_month = monthly_stats['mean'].idxmax()
low_month = monthly_stats['mean'].idxmin()
print(f"\n→ Peak month: {peak_month} ({monthly_stats.loc[peak_month, 'mean']:,.0f} MWh)")
print(f"→ Lowest month: {low_month} ({monthly_stats.loc[low_month, 'mean']:,.0f} MWh)")

print("[8] CORRELATION BETWEEN REGIONS")
print("-"*100)
regions_cols = [
    'National Hourly Demand',
    'Northen Region Hourly Demand',
    'Western Region Hourly Demand',
    'Eastern Region Hourly Demand',
    'Southern Region Hourly Demand',
    'North-Eastern Region Hourly Demand'
]
correlation_matrix = df[regions_cols].corr()
correlation_matrix.index = ['National', 'North', 'West', 'East', 'South', 'NE']
correlation_matrix.columns = ['National', 'North', 'West', 'East', 'South', 'NE']
print(correlation_matrix.round(4).to_string())

print("[9] OUTLIER & DISTRIBUTION ANALYSIS")
demand = df['National Hourly Demand']
Q1 = demand.quantile(0.25)
Q3 = demand.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = demand[(demand < lower_bound) | (demand > upper_bound)]

print(f"IQR Method: Outliers detected: {len(outliers_iqr)}")
z_scores = np.abs(stats.zscore(demand))
outliers_zscore = (z_scores > 3).sum()
print(f"Z-score Method (|Z| > 3): Outliers detected: {outliers_zscore}")

print("[4] DATA QUALITY")
print("-"*100)
missing_count = df.isnull().sum()
missing_pct = (missing_count / len(df)) * 100
for col, count, pct in zip(df.columns, missing_count, missing_pct):
    if count > 0:
        print(f"  {col}: {count} ({pct:.2f}%)")
if missing_count.sum() == 0:
    print("  ✓ No missing values found!")
print(f"\nDuplicate rows: {df.duplicated().sum()}")
print(f"Duplicate datetime: {df['datetime'].duplicated().sum()}")

numeric_cols = df.select_dtypes(include=[np.number]).columns
negative_cols = {col: (df[col] < 0).sum() for col in numeric_cols if (df[col] < 0).sum() > 0}
if negative_cols:
    print("\nNegative values (should be 0):")
    for col, count in negative_cols.items():
        print(f"  {col}: {count}")
else:
    print("\n✓ No negative values found!")
print("✓ DATA QUALITY: EXCELLENT - No issues found!\n")

"""DATA PRE-PROCESSING and FEATURE ENGINEERING"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Load data
df = pd.read_excel('hourlyLoadDataIndia.xlsx')
df['datetime'] = pd.to_datetime(df['datetime'])
df = df.sort_values('datetime').reset_index(drop=True)

# 1. Rename columns for easier access
df = df.rename(columns={
    'National Hourly Demand': 'National_Demand',
    'Northen Region Hourly Demand': 'North_Demand',
    'Western Region Hourly Demand': 'West_Demand',
    'Eastern Region Hourly Demand': 'East_Demand',
    'Southern Region Hourly Demand': 'South_Demand',
    'North-Eastern Region Hourly Demand': 'NE_Demand'
})

# 2. Temporal features
df['hour'] = df['datetime'].dt.hour
df['dayofweek'] = df['datetime'].dt.dayofweek
df['month'] = df['datetime'].dt.month
df['year'] = df['datetime'].dt.year
df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)

# 3. Regional ratios
df['North_ratio'] = df['North_Demand'] / df['National_Demand']
df['West_ratio']  = df['West_Demand']  / df['National_Demand']
df['East_ratio']  = df['East_Demand']  / df['National_Demand']
df['South_ratio'] = df['South_Demand'] / df['National_Demand']
df['NE_ratio']    = df['NE_Demand']    / df['National_Demand']

# 4. Lag features (important for time series)
scaler = MinMaxScaler()
demand_scaled = scaler.fit_transform(df[['National_Demand']]).flatten()
df['lag_1']   = pd.Series(demand_scaled).shift(1)
df['lag_24']  = pd.Series(demand_scaled).shift(24)
df['lag_168'] = pd.Series(demand_scaled).shift(168)
df['lag_336'] = pd.Series(demand_scaled).shift(336)
df['lag_672'] = pd.Series(demand_scaled).shift(672)

# 5. Moving average (optional, 28 days = 672 hours)
df['ma_672'] = pd.Series(demand_scaled).rolling(window=672, min_periods=1).mean()

# 6. Remove NA rows (because of lag/rolling features)
df_clean = df.dropna().reset_index(drop=True)

# 7. Feature set and normalization
feature_cols = [
    'hour', 'dayofweek', 'month', 'is_weekend',
    'North_ratio', 'West_ratio', 'East_ratio', 'South_ratio', 'NE_ratio',
    'lag_1', 'lag_24', 'lag_168', 'lag_336', 'lag_672', 'ma_672'
]
X = df_clean[feature_cols].values

# Target
y = df_clean['National_Demand'].values

# Normalize features
scaler_X = MinMaxScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = MinMaxScaler()
y_scaled = scaler_y.fit_transform(y.reshape(-1,1)).flatten()

# 8. Train-test split (chronologically, NOT shuffled)
split = int(0.8 * len(X_scaled))
X_train, X_test = X_scaled[:split], X_scaled[split:]
y_train, y_test = y_scaled[:split], y_scaled[split:]

print(f"X_train: {X_train.shape} | y_train: {y_train.shape}")
print(f"X_test: {X_test.shape} | y_test: {y_test.shape}")
print("✓ Data preprocessing and feature engineering complete. Ready for modeling!")

"""XGBoost"""

# Install XGBoost if not already installed (uncomment if needed)
# !pip install xgboost

import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Train XGBoost Regressor
model_xgb = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    tree_method='hist' # fast for CPU
)

model_xgb.fit(X_train, y_train)
print("✓ XGBoost model trained!")

# Predict and inverse-transform to get original MWh values
y_test_pred_scaled = model_xgb.predict(X_test)
y_train_pred_scaled = model_xgb.predict(X_train)

# If you normalized y, do this to get back to MWh:
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()
y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()

# Evaluate
def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    # "Accuracy" as 100% - MAPE for regression
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")


print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

# Plot first 200 actual vs predicted points
import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted', lw=2)
plt.title('XGBoost - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

"""  MLR"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Train MLR (Multiple Linear Regression)
mlr = LinearRegression()
mlr.fit(X_train, y_train)
print("✓ MLR model trained!")

# Predict and inverse-transform
y_train_pred_scaled = mlr.predict(X_train)
y_test_pred_scaled = mlr.predict(X_test)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")

print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

# (Optional) Plot
import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted (MLR)', lw=2)
plt.title('MLR - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

"""KNN"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Train KNN regressor
knn = KNeighborsRegressor(n_neighbors=5, weights='distance', n_jobs=-1)
knn.fit(X_train, y_train)
print("✓ KNN model trained!")

# Predict and inverse-transform
y_train_pred_scaled = knn.predict(X_train)
y_test_pred_scaled = knn.predict(X_test)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")

print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted (KNN)', lw=2)
plt.title('KNN - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=8,
    min_samples_leaf=4,
    n_jobs=-1,
    random_state=42
)
rf.fit(X_train, y_train)
print("✓ Random Forest model trained!")

y_train_pred_scaled = rf.predict(X_train)
y_test_pred_scaled = rf.predict(X_test)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")

print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted (RF)', lw=2)
plt.title('Random Forest - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Train SVR (Support Vector Regression) with RBF kernel
svr = SVR(kernel='rbf', C=1.0, epsilon=0.02, gamma='scale')
svr.fit(X_train, y_train)
print("✓ SVR model trained!")

# Predict and inverse-transform
y_train_pred_scaled = svr.predict(X_train)
y_test_pred_scaled = svr.predict(X_test)

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")

print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted (SVR)', lw=2)
plt.title('SVR - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

"""MLP"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Build a simple MLP model
mlp = Sequential()
mlp.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
mlp.add(Dropout(0.1))
mlp.add(Dense(32, activation='relu'))
mlp.add(Dropout(0.1))
mlp.add(Dense(1, activation='linear'))

mlp.compile(optimizer='adam', loss='mse', metrics=['mae'])

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = mlp.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=2
)

print("✓ MLP model trained!")

# Predict and transform back
y_train_pred_scaled = mlp.predict(X_train).flatten()
y_test_pred_scaled = mlp.predict(X_test).flatten()

y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1,1)).flatten()
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1,1)).flatten()
y_train_true = scaler_y.inverse_transform(y_train.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test.reshape(-1,1)).flatten()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")

print_metrics(y_train_true, y_train_pred, name="Train")
print_metrics(y_test_true, y_test_pred, name="Test")

import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_test_pred[:200], label='Predicted (MLP)', lw=2)
plt.title('MLP (NN) - Actual vs Predicted National Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

"""VMD-KGBoost-LSTM"""

import os
os.system("pip install vmdpy")


from vmdpy import VMD

alpha = 2000
tau = 0
K = 4  # Number of modes (can tune: 3-6 typical)
DC = 0
init = 1
tol = 1e-7

modes, _, _ = VMD(df['National_Demand'].values, alpha, tau, K, DC, init, tol)
for i in range(K):
    df_clean[f'VMD_Mode_{i+1}'] = modes[i][len(modes[i]) - len(df_clean):]

feature_cols_vmd = feature_cols + [f'VMD_Mode_{i+1}' for i in range(K)]
X_all = df_clean[feature_cols_vmd].values
y_all = df_clean['National_Demand'].values

"""Cell 4: XGBoost as meta-feature"""

import xgboost as xgb

xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)
xgb_model.fit(X_all[:int(0.8*len(X_all))], y_all[:int(0.8*len(y_all))])
xgb_pred_full = xgb_model.predict(X_all)
df_clean['XGB_Pred'] = xgb_pred_full

"""LSTM Data Preparation"""

timesteps = 24  # Use past 24 hours
feature_cols_final = feature_cols_vmd + ['XGB_Pred']
X_lstm_full = df_clean[feature_cols_final].values
y_lstm_full = df_clean['National_Demand'].values

def create_lstm_data(X, y, timesteps):
    Xs, ys = [], []
    for i in range(timesteps, len(X)):
        Xs.append(X[i-timesteps:i, :])
        ys.append(y[i])
    return np.array(Xs), np.array(ys)

X_lstm, y_lstm = create_lstm_data(X_lstm_full, y_lstm_full, timesteps)
split = int(0.8 * len(X_lstm))
X_train_lstm, X_test_lstm = X_lstm[:split], X_lstm[split:]
y_train_lstm, y_test_lstm = y_lstm[:split], y_lstm[split:]

# LSTM Model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer

model = Sequential()
model.add(InputLayer(input_shape=(timesteps, X_train_lstm.shape[2])))
model.add(LSTM(64, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(32))
model.add(Dropout(0.2))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mae')
model.fit(X_train_lstm, y_train_lstm, validation_split=0.1, batch_size=32, epochs=20, verbose=2)

y_pred_lstm = model.predict(X_test_lstm).flatten()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def print_metrics(y_true, y_pred, name="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    accuracy = 100 - mape
    print(f"{name} MAE:   {mae:,.2f} MWh")
    print(f"{name} RMSE:  {rmse:,.2f} MWh")
    print(f"{name} MAPE:  {mape:.2f}%")
    print(f"{name} R^2:   {r2:.4f}")
    print(f"{name} Accuracy: {accuracy:.2f}%")

print_metrics(y_test_lstm, y_pred_lstm, name="Test (VMD-XGBoost-LSTM)")

import matplotlib.pyplot as plt
plt.figure(figsize=(15,4))
plt.plot(y_test_lstm[:200], label='Actual', lw=2)
plt.plot(y_pred_lstm[:200], label='Predicted (Hybrid)', lw=2)
plt.title('VMD-XGBoost-LSTM Hybrid - Actual vs Predicted Demand (first 200 test hours)')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

# ---------- Unified, corrected VMD -> XGBoost -> LSTM pipeline ----------
import numpy as np
import pandas as pd
from vmdpy import VMD
from sklearn.preprocessing import MinMaxScaler
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# assume df is already loaded, sorted, and renamed earlier in your notebook:
# df['datetime'] sorted, df['National_Demand'] present.

# 0) Keep a raw copy
df['National_Demand_raw'] = df['National_Demand'].astype(float)

# 1) Compute VMD ON FULL RAW SERIES (so mode indexing is simple)
alpha = 2000
tau = 0
K = 4
DC = 0
init = 1
tol = 1e-7

modes, _, _ = VMD(df['National_Demand_raw'].values, alpha, tau, K, DC, init, tol)
# modes is shape (K, N). Attach them as columns aligned with df index
for i in range(K):
    df[f'VMD_Mode_{i+1}_raw'] = modes[i]

# 2) Create lag features and moving averages FROM RAW DEMAND (no scaling yet)
df['hour'] = df['datetime'].dt.hour
df['dayofweek'] = df['datetime'].dt.dayofweek
df['month'] = df['datetime'].dt.month
df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)

df['North_ratio'] = df['North_Demand'] / df['National_Demand_raw']
df['West_ratio']  = df['West_Demand']  / df['National_Demand_raw']
df['East_ratio']  = df['East_Demand']  / df['National_Demand_raw']
df['South_ratio'] = df['South_Demand'] / df['National_Demand_raw']
df['NE_ratio']    = df['NE_Demand']   / df['National_Demand_raw']

# lags in raw MWh
df['lag_1']   = df['National_Demand_raw'].shift(1)
df['lag_24']  = df['National_Demand_raw'].shift(24)
df['lag_168'] = df['National_Demand_raw'].shift(168)
df['lag_336'] = df['National_Demand_raw'].shift(336)
df['lag_672'] = df['National_Demand_raw'].shift(672)
df['ma_672']  = df['National_Demand_raw'].rolling(window=672, min_periods=1).mean()

# dropna -> align VMD modes and everything
df_clean = df.dropna().reset_index(drop=True)

# 3) Build feature columns (use raw VMD modes and raw lags; we'll scale them after split)
vmd_cols = [f'VMD_Mode_{i+1}_raw' for i in range(K)]
feature_cols_vmd = [
    'hour', 'dayofweek', 'month', 'is_weekend',
    'North_ratio', 'West_ratio', 'East_ratio', 'South_ratio', 'NE_ratio',
    'lag_1', 'lag_24', 'lag_168', 'lag_336', 'lag_672', 'ma_672'
] + vmd_cols

X_all_raw = df_clean[feature_cols_vmd].values
y_all_raw = df_clean['National_Demand_raw'].values.reshape(-1, 1)

# 4) Chronological split indexes (80%)
split_idx = int(0.8 * len(X_all_raw))
X_train_raw = X_all_raw[:split_idx]
X_test_raw  = X_all_raw[split_idx:]
y_train_raw = y_all_raw[:split_idx]
y_test_raw  = y_all_raw[split_idx:]

# 5) Fit scalers on TRAIN only
scaler_X = MinMaxScaler()
X_train = scaler_X.fit_transform(X_train_raw)
X_test  = scaler_X.transform(X_test_raw)
X_all_scaled = np.vstack([X_train, X_test])  # if needed

scaler_y = MinMaxScaler()
y_train = scaler_y.fit_transform(y_train_raw).flatten()
y_test  = scaler_y.transform(y_test_raw).flatten()

# 6) XGBoost on scaled train data
xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42, tree_method='hist')
xgb_model.fit(X_train, y_train)

# 7) Create XGB predictions (scaled) for all rows - used as a feature for LSTM
xgb_pred_train_scaled = xgb_model.predict(X_train)
xgb_pred_test_scaled  = xgb_model.predict(X_test)
xgb_pred_full_scaled  = np.concatenate([xgb_pred_train_scaled, xgb_pred_test_scaled])

# append scaled XGB predictions to scaled X arrays to form final features for LSTM
X_train_with_xgb = np.concatenate([X_train, xgb_pred_train_scaled.reshape(-1,1)], axis=1)
X_test_with_xgb  = np.concatenate([X_test,  xgb_pred_test_scaled.reshape(-1,1)], axis=1)
X_all_with_xgb_scaled = np.concatenate([X_all_scaled, xgb_pred_full_scaled.reshape(-1,1)], axis=1)

# 8) Create LSTM sequences (timesteps = 24)
timesteps = 24

def create_sequences(X_scaled, y_scaled, timesteps):
    Xs, ys = [], []
    for i in range(timesteps, len(X_scaled)):
        Xs.append(X_scaled[i-timesteps:i, :])
        ys.append(y_scaled[i])
    return np.array(Xs), np.array(ys)

# BUT sequences must be created from the FULL scaled dataset with XGB feature included,
# then split chronologically (so there's no leakage).
X_seq_full, y_seq_full = create_sequences(X_all_with_xgb_scaled, np.concatenate([y_train, y_test]), timesteps)

# compute new split index for sequences (because we lost 'timesteps' rows at the start)
seq_split = int(0.8 * len(X_seq_full))
X_train_lstm = X_seq_full[:seq_split]
X_test_lstm  = X_seq_full[seq_split:]
y_train_lstm = y_seq_full[:seq_split]
y_test_lstm  = y_seq_full[seq_split:]

print("Shapes ->", X_train_lstm.shape, y_train_lstm.shape, X_test_lstm.shape, y_test_lstm.shape)

# 9) Build LSTM (on scaled data)
model = Sequential([
    InputLayer(input_shape=(timesteps, X_train_lstm.shape[2])),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1, activation='linear')
])
model.compile(optimizer='adam', loss='mae')
history = model.fit(X_train_lstm, y_train_lstm, validation_split=0.1, epochs=30, batch_size=64, verbose=2, shuffle=False)

# 10) Predict (scaled) then inverse transform to MWh
y_pred_test_scaled = model.predict(X_test_lstm).flatten()
# inverse transform using scaler_y
y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test_lstm.reshape(-1,1)).flatten()

# 11) Metrics (use small epsilon for stability in MAPE)
eps = 1e-6
def metrics(y_true, y_pred, label="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100
    r2 = r2_score(y_true, y_pred)
    acc = 100 - mape
    print(f"{label} MAE:   {mae:,.2f} MWh")
    print(f"{label} RMSE:  {rmse:,.2f} MWh")
    print(f"{label} MAPE:  {mape:.2f}%")
    print(f"{label} R^2:   {r2:.4f}")
    print(f"{label} Accuracy: {acc:.2f}%")

metrics(y_test_true, y_pred_test, label="Test (Corrected VMD-XGB-LSTM)")

# 12) Quick plot first 200 points
plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_pred_test[:200], label='Predicted (Hybrid corrected)', lw=2)
plt.title('Corrected VMD-XGBoost-LSTM - first 200 test hours')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()
# -----------------------------------------------------------------------

print("X_train_lstm shape:", X_train_lstm.shape)
print("First input sequence (flattened):", X_train_lstm[0].flatten())
print("First target value:", y_train_lstm[0])
print("Typical y_train_lstm range:", y_train_lstm.min(), y_train_lstm.max())

"""XGBoost-LSTM

"""

#. Feature Preparation (after EDA, lag, ratios etc.)
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Raw feature matrix (no VMD modes)
feature_cols = [
    'hour', 'dayofweek', 'month', 'is_weekend',
    'North_ratio', 'West_ratio', 'East_ratio', 'South_ratio', 'NE_ratio',
    'lag_1', 'lag_24', 'lag_168', 'lag_336', 'lag_672', 'ma_672'
]
X_all_raw = df_clean[feature_cols].values
y_all_raw = df_clean['National_Demand_raw'].values.reshape(-1, 1)

split_idx = int(0.8 * len(X_all_raw))
X_train_raw, X_test_raw = X_all_raw[:split_idx], X_all_raw[split_idx:]
y_train_raw, y_test_raw = y_all_raw[:split_idx], y_all_raw[split_idx:]

scaler_X = MinMaxScaler()
X_train = scaler_X.fit_transform(X_train_raw)
X_test  = scaler_X.transform(X_test_raw)
X_all_scaled = np.vstack([X_train, X_test])

scaler_y = MinMaxScaler()
y_train = scaler_y.fit_transform(y_train_raw).flatten()
y_test  = scaler_y.transform(y_test_raw).flatten()

# Train XGBoost and Build Meta-feature
import xgboost as xgb

xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42, tree_method='hist')
xgb_model.fit(X_train, y_train)

xgb_pred_train_scaled = xgb_model.predict(X_train)
xgb_pred_test_scaled  = xgb_model.predict(X_test)
xgb_pred_full_scaled  = np.concatenate([xgb_pred_train_scaled, xgb_pred_test_scaled])

# Append meta-feature to all arrays
X_train_with_xgb = np.concatenate([X_train, xgb_pred_train_scaled.reshape(-1,1)], axis=1)
X_test_with_xgb  = np.concatenate([X_test,  xgb_pred_test_scaled.reshape(-1,1)], axis=1)
X_all_with_xgb_scaled = np.concatenate([X_all_scaled, xgb_pred_full_scaled.reshape(-1,1)], axis=1)

# LSTM Sequence Creation
timesteps = 24

def create_sequences(X_scaled, y_scaled, timesteps):
    Xs, ys = [], []
    for i in range(timesteps, len(X_scaled)):
        Xs.append(X_scaled[i-timesteps:i, :])
        ys.append(y_scaled[i])
    return np.array(Xs), np.array(ys)

X_seq_full, y_seq_full = create_sequences(X_all_with_xgb_scaled, np.concatenate([y_train, y_test]), timesteps)

seq_split = int(0.8 * len(X_seq_full))
X_train_lstm = X_seq_full[:seq_split]
X_test_lstm  = X_seq_full[seq_split:]
y_train_lstm = y_seq_full[:seq_split]
y_test_lstm  = y_seq_full[seq_split:]

# LSTM Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer

model = Sequential([
    InputLayer(input_shape=(timesteps, X_train_lstm.shape[2])),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1, activation='linear')
])
model.compile(optimizer='adam', loss='mae')
history = model.fit(X_train_lstm, y_train_lstm, validation_split=0.1, epochs=30, batch_size=64, verbose=2, shuffle=False)

y_pred_test_scaled = model.predict(X_test_lstm).flatten()
y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1,1)).flatten()
y_test_true = scaler_y.inverse_transform(y_test_lstm.reshape(-1,1)).flatten()

def metrics(y_true, y_pred, label="Test"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-6))) * 100
    r2 = r2_score(y_true, y_pred)
    acc = 100 - mape
    print(f"{label} MAE:   {mae:,.2f} MWh")
    print(f"{label} RMSE:  {rmse:,.2f} MWh")
    print(f"{label} MAPE:  {mape:.2f}%")
    print(f"{label} R^2:   {r2:.4f}")
    print(f"{label} Accuracy: {acc:.2f}%")

metrics(y_test_true, y_pred_test, label="Test (XGBoost-LSTM Stacking)")

plt.figure(figsize=(15,4))
plt.plot(y_test_true[:200], label='Actual', lw=2)
plt.plot(y_pred_test[:200], label='Predicted (XGB-LSTM Stack)', lw=2)
plt.title('XGBoost-LSTM Stacking - first 200 test hours')
plt.xlabel('Hour')
plt.ylabel('Demand (MWh)')
plt.legend()
plt.tight_layout()
plt.show()

"""Sparse Transformer + CNN + LSTM Hybrid Model"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, LSTM, Concatenate, LayerNormalization
from tensorflow.keras.models import Model

# You may need to install keras-transformer or use the built-in MultiHeadAttention for the transformer block
from tensorflow.keras.layers import MultiHeadAttention

def build_sparse_transformer_cnn_lstm(timesteps, features, num_heads=2, ff_dim=64):
    inputs = Input(shape=(timesteps, features))

    # --- CNN Branch: Local Feature Extraction ---
    x_cnn = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)
    x_cnn = Dropout(0.2)(x_cnn)

    # --- Transformer Branch: Long-Term Patterns ---
    # Layer normalization for attention input
    attn_input = LayerNormalization(epsilon=1e-6)(inputs)
    attn_out = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim, dropout=0.2)(attn_input, attn_input)
    attn_out = Dropout(0.2)(attn_out)
    attn_out = LayerNormalization(epsilon=1e-6)(attn_out)
    # Simple Feed-Forward
    ff_out = Dense(ff_dim, activation='relu')(attn_out)

    # --- LSTM: Sequential Temporal Modeling ---
    x_lstm = LSTM(64, return_sequences=False, dropout=0.2)(inputs)

    # --- Concatenate All Branches ---
    x_cnn_flat = tf.keras.layers.Flatten()(x_cnn)
    ff_flat = tf.keras.layers.Flatten()(ff_out)
    x_concat = Concatenate()([x_cnn_flat, ff_flat, x_lstm])

    # --- Final Output Layers ---
    x_out = Dense(64, activation='relu')(x_concat)
    x_out = Dropout(0.2)(x_out)
    outputs = Dense(1, activation='linear')(x_out)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='mae', metrics=['mae', 'mse'])
    return model

# Assuming you have X_train_seq, y_train_seq, X_val_seq, y_val_seq
# Example shapes: X_train_seq.shape = (num_samples, 24, feature_dim)
timesteps = X_train_seq.shape[1]
features = X_train_seq.shape[2]

model = build_sparse_transformer_cnn_lstm(timesteps, features)
model.summary()

history = model.fit(
    X_train_seq, y_train_seq,
    validation_data=(X_val_seq, y_val_seq),
    epochs=50,
    batch_size=64,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],
    verbose=2
)

# Predict
y_pred = model.predict(X_test_seq).flatten()















